{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aa22291-722e-429c-926b-4ec73581a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is adapted from [How I Created an AI Version of Myself] by Keith McNulty\n",
    "# Original repository: https://github.com/keithmcnulty/rag_gemma_regression_book/blob/main/rag_gemma_experiment_open.ipynb\n",
    "# Modifications by Aleksand Botvin\n",
    "# I have made the following modifications:\n",
    "#  - Read PDF files\n",
    "#  - Using serverless LLM\n",
    "#  - Creating a Telegram bot as an interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2376d28f-19f6-4151-b92d-8523c810fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем библиотеку для работы с ChromaDB\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.utils.batch_utils import create_batches\n",
    "\n",
    "# Импортируем клиент OpenAI для работы с API OpenRouter\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ed6fd3-f0db-4af5-81dd-8f9e98ff353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка необходимых пакетов:\n",
    "# pip install python-telegram-bot\n",
    "\n",
    "# Подключаем пакеты для работы с Telegram\n",
    "import logging\n",
    "from telegram import Update\n",
    "from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters\n",
    "\n",
    "# Для работы с регулярными выражениями\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "473dd0f6-f4bd-4a50-aff4-7e0d39833cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт и применение nest_asyncio для обеспечения корректной работы асинхронного кода в Jupyter или других окружениях\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d307244-948b-47e5-8957-d840875390c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alexa\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Указываем путь к локальной базе данных ChromaDB\n",
    "CHROMA_DATA_PATH = \"chroma_data_bayesium/\"\n",
    "\n",
    "# Используем предобученную модель для получения эмбеддингов\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Название коллекции, в которой хранятся документы\n",
    "COLLECTION_NAME = \"bayesium\"\n",
    "\n",
    "# Инициализируем клиента ChromaDB для работы с персистентным хранилищем\n",
    "chromadb_client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)\n",
    "\n",
    "# Переинициализируем функцию эмбеддинга с использованием выбранной модели\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=EMBED_MODEL\n",
    ")\n",
    "\n",
    "# Получаем уже созданную коллекцию с книгами по байесовской статистике\n",
    "collection = chromadb_client.get_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_func  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4375f578-baac-435e-92ce-b88350d23496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём клиента OpenAI для доступа к API OpenRouter \n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=\"<ВАШ API KEY>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82f2b96c-7409-4f26-8517-aa3bd434f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str, n_docs: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Функция для обработки вопроса.\n",
    "    \n",
    "    Аргументы:\n",
    "      question (str): Текст вопроса.\n",
    "      n_docs (int): Количество документов, извлекаемых из базы для формирования ответа.\n",
    "      \n",
    "    Возвращает:\n",
    "      str: Сгенерированный ответ LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Получаем коллекцию из базы ChromaDB\n",
    "    collection = chromadb_client.get_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        embedding_function=embedding_func  \n",
    "    )\n",
    "    \n",
    "    # Выполняем поиск по схожести: получаем n_docs наиболее релевантных документов\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=n_docs\n",
    "    )\n",
    "    \n",
    "    # Формируем контекст с текстом документа и соответствующими метаданными (глава, источник)\n",
    "    context_with_metadata = \"\"\n",
    "    referenced_chapters = set()  # Множество для хранения уникальных названий глав\n",
    "    referenced_sources = set()   # Множество для хранения уникальных источников\n",
    "\n",
    "    # Проходим по найденным документам\n",
    "    for i, document in enumerate(results['documents'][0]):\n",
    "        # Получаем метаданные для текущего документа\n",
    "        metadata = results['metadatas'][0][i]  \n",
    "        chapter = metadata.get('chapter', 'Unknown Chapter')  \n",
    "        referenced_chapters.add(chapter) \n",
    "        source = metadata.get('source') \n",
    "        referenced_sources.add(source)\n",
    "        \n",
    "        # Формируем строку с информацией о документе\n",
    "        context_with_metadata += f\"Document {i+1} (Chapter: {chapter}):\\n\"\n",
    "        context_with_metadata += f\"{document}\\n\"\n",
    "        context_with_metadata += \"-\" * 50 + \"\\n\"\n",
    "\n",
    "    # Объединяем названия глав и источники в строку для дальнейшего упоминания в ответе\n",
    "    chapters_list = \", \".join(referenced_chapters)\n",
    "    sources = \", \".join(referenced_sources)\n",
    "\n",
    "    # Формируем окончательный запрос для LLM, включающий вопрос и контекст\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert on statistics and its applications to analytics.\n",
    "    Here is a question: {question}\n",
    "    Answer it strictly based on the following information:\n",
    "    {context_with_metadata}\n",
    "    When providing the answer, reference the chapters used by stating \"This information is from {chapters_list} by {sources}\".\n",
    "    If the context does not provide enough information, say \"I cannot answer this question based on the provided context.\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Оборачиваем запрос в формате сообщений для модели\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Отправляем запрос на генерацию ответа LLM с помощью OpenRouter\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3.1-70b-instruct:free\", \n",
    "        messages=messages,\n",
    "        max_tokens=1000,\n",
    "        temperature=0.25 \n",
    "    )\n",
    "\n",
    "    # Возвращаем сгенерированный ответ\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3e97d9b-7d17-46ea-a256-1364995b7df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разрешённый ID пользователя, которому разрешено использовать бота\n",
    "ALLOWED_USER_ID = 164935376\n",
    "\n",
    "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    \"\"\"\n",
    "    Обрабатывает команду /start:\n",
    "      - Проверяет, является ли пользователь авторизованным.\n",
    "      - Отправляет приветственное сообщение.\n",
    "    \"\"\"\n",
    "    user_id = update.effective_user.id\n",
    "    if user_id != ALLOWED_USER_ID:\n",
    "        await update.message.reply_text(\"Извините, но вы не можете воспользоваться этим ботом.\")\n",
    "        return\n",
    "\n",
    "    await update.message.reply_text(\n",
    "        \"Привет! Я Байезиум - бот по байесовской статистике. Задай мне вопрос и я отвечу на основе книг Ричарда МакЭлрита и Джона Крушке\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41ecf36a-8531-49eb-bcdf-facb677fdda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_code_for_telegram(text: str, parse_mode: str = \"HTML\") -> str:\n",
    "    \"\"\"\n",
    "    Форматирует текст для Telegram с использованием HTML:\n",
    "      - Экранирует специальные HTML-символы.\n",
    "      - Преобразует блоки кода, выделенные с помощью ```...```, в HTML-теги <pre><code>...</code></pre>.\n",
    "    \"\"\"\n",
    "    if parse_mode == \"HTML\":\n",
    "        # Экранирование символов &, <, >\n",
    "        text = text.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
    "        # Замена блоков кода на HTML-формат\n",
    "        text = re.sub(r\"```(\\w+)?\\n(.*?)```\", r\"<pre><code>\\2</code></pre>\", text, flags=re.DOTALL)\n",
    "        return text\n",
    "    else:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc4e84b3-5ba4-40a9-b9f3-97c7d7080637",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    \"\"\"\n",
    "    Обрабатывает входящие сообщения:\n",
    "      - Проверяет авторизацию пользователя.\n",
    "      - Извлекает текст вопроса из сообщения.\n",
    "      - Уведомляет пользователя о начале обработки.\n",
    "      - Вызывает функцию ask_question для получения ответа.\n",
    "      - Форматирует ответ и отправляет его обратно пользователю.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_id = update.effective_user.id\n",
    "    if user_id != ALLOWED_USER_ID:\n",
    "        await update.message.reply_text(\"Извините, но вы не можете воспользоваться этим ботом.\")\n",
    "        return\n",
    "\n",
    "    question = update.message.text\n",
    "    print(f\"DEBUG: Received message - {question}\")\n",
    "    logging.info(f\"DEBUG: Received message - {question}\")\n",
    "\n",
    "    # Информируем пользователя о том, что запрос обрабатывается\n",
    "    await update.message.reply_text(\"Осуществляется обработка вашего вопроса, пожалуйста, ожидайте...\")\n",
    "\n",
    "    try:\n",
    "        # Выполнение функции ask_question в отдельном потоке, чтобы не блокировать event loop\n",
    "        answer = await asyncio.to_thread(ask_question, question)\n",
    "        print(f\"DEBUG: Answer generated - {answer}\")\n",
    "        logging.info(f\"DEBUG: Answer generated - {answer}\")\n",
    "\n",
    "        # Форматирование ответа для корректного отображения в Telegram\n",
    "        formatted_answer = format_code_for_telegram(answer, parse_mode=\"HTML\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in ask_question: {e}\")\n",
    "        formatted_answer = \"Sorry, something went wrong while processing your request.\"\n",
    "\n",
    "    # Отправка отформатированного ответа пользователю\n",
    "    await update.message.reply_text(formatted_answer, parse_mode=\"HTML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ec6fd-0d8a-4673-b1d1-07b5c3cd2357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 16:17:57,099 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot/getMe \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 16:17:57,189 - httpx - INFO - HTTP Request: POST https://api.telegram.org/deleteWebhook \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 16:17:57,190 - telegram.ext.Application - INFO - Application started\n",
      "2025-02-15 16:18:07,736 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot/getUpdates \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "async def main() -> None:\n",
    "    \"\"\"\n",
    "    Основная функция:\n",
    "      - Создает экземпляр приложения Telegram-бота.\n",
    "      - Регистрирует обработчики команд и сообщений.\n",
    "      - Запускает цикл поллинга для получения обновлений.\n",
    "    \"\"\"\n",
    "    application = ApplicationBuilder().token(\"<ВАШ API KEY>\").build()\n",
    "\n",
    "    # Регистрация обработчика команды /start\n",
    "    application.add_handler(CommandHandler(\"start\", start))\n",
    "    # Регистрация обработчика для всех остальных сообщений\n",
    "    application.add_handler(MessageHandler(filters.ALL, handle_message))\n",
    "\n",
    "    # Запуск бота в режиме поллинга\n",
    "    await application.run_polling()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Настройка логирования для отслеживания событий и ошибок\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO\n",
    "    )\n",
    "\n",
    "    import asyncio\n",
    "    # Запуск основной функции с использованием asyncio\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0aa5d-456f-46c0-a17a-24e82701fe3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
