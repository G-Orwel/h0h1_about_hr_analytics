{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a9c9ed0-3d93-44b6-b526-17ce8ebe3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is adapted from [How I Created an AI Version of Myself] by Keith McNulty\n",
    "# Original repository: https://github.com/keithmcnulty/rag_gemma_regression_book/blob/main/rag_gemma_experiment_open.ipynb\n",
    "# Modifications by Aleksand Botvin\n",
    "# I have made the following modifications:\n",
    "#  - Read PDF files\n",
    "#  - Using serverless LLM\n",
    "#  - Creating a Telegram bot as an interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f226e73d-fccb-4146-9149-4ea37a69db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка необходимых пакетов:\n",
    "# pip install openai\n",
    "\n",
    "# Импортируем библиотеку для работы с ChromaDB\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.utils.batch_utils import create_batches\n",
    "\n",
    "# Импортируем клиент OpenAI для работы с API OpenRouter\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a77df94e-6c8a-4c04-b157-f5cb14e91a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Указываем путь к локальной базе данных ChromaDB\n",
    "CHROMA_DATA_PATH = \"chroma_data_bayesium/\"\n",
    "\n",
    "# Используем предобученную модель для получения эмбеддингов\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Название коллекции, в которой хранятся документы\n",
    "COLLECTION_NAME = \"bayesium\"\n",
    "\n",
    "# Инициализируем клиента ChromaDB для работы с персистентным хранилищем\n",
    "chromadb_client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)\n",
    "\n",
    "# Переинициализируем функцию эмбеддинга с использованием выбранной модели\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=EMBED_MODEL\n",
    ")\n",
    "\n",
    "# Получаем уже созданную коллекцию с книгами по байесовской статистике\n",
    "collection = chromadb_client.get_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_func  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f76e04a-0530-47ea-81cb-618a602276c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём клиента OpenAI для доступа к API OpenRouter \n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=\"<ВАШ API KEY>\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d056ea3-ee69-4bec-b056-4940efb4d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str, n_docs: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Функция для обработки вопроса.\n",
    "    \n",
    "    Аргументы:\n",
    "      question (str): Текст вопроса.\n",
    "      n_docs (int): Количество документов, извлекаемых из базы для формирования ответа.\n",
    "      \n",
    "    Возвращает:\n",
    "      str: Сгенерированный ответ LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Получаем коллекцию из базы ChromaDB\n",
    "    collection = chromadb_client.get_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        embedding_function=embedding_func  \n",
    "    )\n",
    "    \n",
    "    # Выполняем поиск по схожести: получаем n_docs наиболее релевантных документов\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=n_docs\n",
    "    )\n",
    "    \n",
    "    # Формируем контекст с текстом документа и соответствующими метаданными (глава, источник)\n",
    "    context_with_metadata = \"\"\n",
    "    referenced_chapters = set()  # Множество для хранения уникальных названий глав\n",
    "    referenced_sources = set()   # Множество для хранения уникальных источников\n",
    "\n",
    "    # Проходим по найденным документам\n",
    "    for i, document in enumerate(results['documents'][0]):\n",
    "        # Получаем метаданные для текущего документа\n",
    "        metadata = results['metadatas'][0][i]  \n",
    "        chapter = metadata.get('chapter', 'Unknown Chapter')  \n",
    "        referenced_chapters.add(chapter) \n",
    "        source = metadata.get('source') \n",
    "        referenced_sources.add(source)\n",
    "        \n",
    "        # Формируем строку с информацией о документе\n",
    "        context_with_metadata += f\"Document {i+1} (Chapter: {chapter}):\\n\"\n",
    "        context_with_metadata += f\"{document}\\n\"\n",
    "        context_with_metadata += \"-\" * 50 + \"\\n\"\n",
    "\n",
    "    # Объединяем названия глав и источники в строку для дальнейшего упоминания в ответе\n",
    "    chapters_list = \", \".join(referenced_chapters)\n",
    "    sources = \", \".join(referenced_sources)\n",
    "\n",
    "    # Формируем окончательный запрос для LLM, включающий вопрос и контекст\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert on statistics and its applications to analytics.\n",
    "    Here is a question: {question}\n",
    "    Answer it strictly based on the following information:\n",
    "    {context_with_metadata}\n",
    "    When providing the answer, reference the chapters used by stating \"This information is from {chapters_list} by {sources}\".\n",
    "    If the context does not provide enough information, say \"I cannot answer this question based on the provided context.\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Оборачиваем запрос в формате сообщений для модели\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Отправляем запрос на генерацию ответа LLM с помощью OpenRouter\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3.1-70b-instruct:free\", \n",
    "        messages=messages,\n",
    "        max_tokens=1000,\n",
    "        temperature=0.25 \n",
    "    )\n",
    "\n",
    "    # Возвращаем сгенерированный ответ\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83153a4a-c46f-4045-810e-08ff9202c36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This information is from Chapter 9: Markov Chain Monte Carlo, Chapter 15: Missing Data and Other Opportunities by McElreath R.\n",
      "\n",
      "To provide an example of regression using the ulam function in R, we can use the terrain ruggedness example from Chapter 9. First, we need to load the data and transform the predictors:\n",
      "\n",
      "```r\n",
      "library(rethinking)\n",
      "data(rugged)\n",
      "d <- rugged\n",
      "d$log_gdp <- log(d$rgdppc_2000)\n",
      "dd <- d[ complete.cases(d$rgdppc_2000) , ]\n",
      "dd$log_gdp_std <- dd$log_gdp / mean(dd$log_gdp)\n",
      "dd$rugged_std <- dd$rugged / max(dd$rugged)\n",
      "dd$cid <- ifelse( dd$cont_africa==1 , 1 , 2 )\n",
      "dat_list <- list(\n",
      "  log_gdp_std = dd$log_gdp_std,\n",
      "  rugged_std = dd$rugged_std,\n",
      "  cid = dd$cid\n",
      ")\n",
      "```\n",
      "\n",
      "Then, we can define the model using ulam:\n",
      "\n",
      "```r\n",
      "m9.1 <- ulam(\n",
      "  alist(\n",
      "    log_gdp_std ~ dnorm(mu, sigma),\n",
      "    mu <- a[cid] + b[cid] * rugged_std,\n",
      "    a[cid] ~ dnorm(0, 0.1),\n",
      "    b[cid] ~ dnorm(0, 0.3),\n",
      "    sigma ~ dexp(1)\n",
      "  ),\n",
      "  data = dat_list,\n",
      "  chains = 4,\n",
      "  cores = 4\n",
      ")\n",
      "```\n",
      "\n",
      "This model estimates the relationship between log GDP and ruggedness, with different intercepts and slopes for each continent. The `ulam` function will then fit the model using Markov Chain Monte Carlo (MCMC) and provide the posterior distribution of the parameters.\n",
      "\n",
      "To summarize the results, we can use the `precis` function:\n",
      "\n",
      "```r\n",
      "precis(m9.1, depth = 2)\n",
      "```\n",
      "\n",
      "This will provide the mean, standard deviation, and 89% credible interval for each parameter in the model.\n"
     ]
    }
   ],
   "source": [
    "question = \"Give an example of regression using the Ulam function in R.\"\n",
    "\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "338139e3-b33f-4b70-aab8-4b86455e327a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot answer this question based on the provided context. The provided chapters from \"Chapter 3: The R Programming Language\" discuss getting help in R, the R programming language, and its user interface, but do not provide information on creating a dashboard. \n",
      "\n",
      "This information is from Chapter 3: The R Programming Language by Krushcke J.K. However, the chapters provided do not contain sufficient information to create a dashboard.\n"
     ]
    }
   ],
   "source": [
    "question = \"Give an example how to create a dashboard\"\n",
    "\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
